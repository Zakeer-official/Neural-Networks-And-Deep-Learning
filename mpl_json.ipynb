{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layers': [{'type': 'input', 'neurons': 4}, {'type': 'hidden', 'neurons': 4, 'activation': 'sigmoid'}, {'type': 'output', 'neurons': 2, 'activation': 'sigmoid'}], 'learning_rate': 0.1, 'epochs': 10000, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def parse_json_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Example usage\n",
    "config = parse_json_config('MLP.json')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [array([[ 0.89240733,  1.15232528, -1.58683502,  1.08868314],\n",
      "       [-0.53985535, -0.21466758,  1.93228834,  0.71542268],\n",
      "       [ 0.25862468,  0.56310304, -1.4396527 , -2.14956421],\n",
      "       [ 1.67937681,  0.86758745, -0.54820952,  1.4371271 ]]), array([[ 0.48588327, -0.61967597],\n",
      "       [ 0.17659387,  0.89786977],\n",
      "       [ 0.04809907,  0.61024777],\n",
      "       [-0.64826161,  0.39791691]])]\n",
      "Biases: [array([[ 1.33530157,  1.08049492,  0.27966992, -0.89818769]]), array([[ 0.10701393, -0.83115166]])]\n",
      "Epoch 0, Loss: 0.2295572954371205\n",
      "Epoch 1000, Loss: 0.002873509364470388\n",
      "Epoch 2000, Loss: 0.00042090153136964013\n",
      "Epoch 3000, Loss: 0.0001469746313982441\n",
      "Epoch 4000, Loss: 7.242176292456313e-05\n",
      "Epoch 5000, Loss: 4.2628700146626425e-05\n",
      "Epoch 6000, Loss: 2.7929234041947914e-05\n",
      "Epoch 7000, Loss: 1.964933757443239e-05\n",
      "Epoch 8000, Loss: 1.454296470684659e-05\n",
      "Epoch 9000, Loss: 1.117944661883051e-05\n",
      "Predictions: [[0.00314134 0.00308356]\n",
      " [0.00203839 0.99730251]\n",
      " [0.99803151 0.00398472]\n",
      " [0.9966688  0.99697589]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, config_path):\n",
    "        # Load configuration from JSON file\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = json.load(file)\n",
    "        \n",
    "        self.layers_config = config.get('layers', [])\n",
    "        self.learning_rate = config.get('learning_rate', 0.01)\n",
    "        self.epochs = config.get('epochs', 1000)\n",
    "        self.batch_size = config.get('batch_size', 32)  # Not used in this implementation, but read in\n",
    "        \n",
    "        self.input_size = None\n",
    "        self.initialize_network()\n",
    "    \n",
    "    def validate_config(self):\n",
    "        \"\"\" Validate the configuration parameters \"\"\"\n",
    "        if not self.layers_config:\n",
    "            raise ValueError(\"The 'layers' configuration is missing or empty.\")\n",
    "        \n",
    "        layer_types = {'input', 'hidden', 'output'}\n",
    "        activations = {'relu', 'sigmoid', 'tanh'}\n",
    "        \n",
    "        for layer in self.layers_config:\n",
    "            layer_type = layer.get('type')\n",
    "            if layer_type not in layer_types:\n",
    "                raise ValueError(f\"Invalid layer type: {layer_type}\")\n",
    "            \n",
    "            if 'neurons' not in layer:\n",
    "                raise ValueError(f\"Layer type '{layer_type}' is missing the 'neurons' key.\")\n",
    "            \n",
    "            if layer_type != 'input' and 'activation' not in layer:\n",
    "                raise ValueError(f\"Layer type '{layer_type}' requires an 'activation' key.\")\n",
    "            \n",
    "            if layer_type != 'input':\n",
    "                activation = layer.get('activation')\n",
    "                if activation not in activations:\n",
    "                    raise ValueError(f\"Invalid activation function: {activation}\")\n",
    "    \n",
    "    def initialize_network(self):\n",
    "        self.validate_config()\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        layer_input_size = None\n",
    "        \n",
    "        for layer in self.layers_config:\n",
    "            layer_type = layer['type']\n",
    "            layer_size = layer['neurons']\n",
    "            \n",
    "            if layer_type == 'input':\n",
    "                self.input_size = layer_size\n",
    "                layer_input_size = self.input_size\n",
    "                continue\n",
    "            \n",
    "            if layer_input_size is None:\n",
    "                raise ValueError(\"Input layer size not defined.\")\n",
    "            \n",
    "            self.weights.append(np.random.randn(layer_input_size, layer_size))\n",
    "            self.biases.append(np.random.randn(1, layer_size))\n",
    "            layer_input_size = layer_size\n",
    "        \n",
    "        # Debug prints to verify initialization\n",
    "        print(\"Weights:\", self.weights)\n",
    "        print(\"Biases:\", self.biases)\n",
    "    \n",
    "    def activation(self, x, func):\n",
    "        if func == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif func == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif func == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {func}\")\n",
    "    \n",
    "    def activation_derivative(self, x, func):\n",
    "        if func == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif func == 'sigmoid':\n",
    "            return x * (1 - x)\n",
    "        elif func == 'tanh':\n",
    "            return 1 - x ** 2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {func}\")\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.z = []\n",
    "        self.a = [X]\n",
    "        \n",
    "        for i in range(len(self.layers_config) - 1):\n",
    "            layer = self.layers_config[i + 1]  # Skip input layer\n",
    "            layer_activation = layer.get('activation', None)\n",
    "            \n",
    "            z = np.dot(self.a[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z.append(z)\n",
    "            a = self.activation(z, layer_activation)\n",
    "            self.a.append(a)\n",
    "        \n",
    "        return self.a[-1]\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        delta = self.a[-1] - y\n",
    "        d_weights = []\n",
    "        d_biases = []\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            d_weights.insert(0, np.dot(self.a[i].T, delta) / m)\n",
    "            d_biases.insert(0, np.sum(delta, axis=0, keepdims=True) / m)\n",
    "            if i != 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(self.a[i], self.layers_config[i]['activation'])\n",
    "        \n",
    "        return d_weights, d_biases\n",
    "    \n",
    "    def update_weights(self, d_weights, d_biases):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * d_weights[i]\n",
    "            self.biases[i] -= self.learning_rate * d_biases[i]\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            output = self.forward_propagation(X)\n",
    "            d_weights, d_biases = self.backward_propagation(X, y)\n",
    "            self.update_weights(d_weights, d_biases)\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean((output - y) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = 'D:\\\\BTech\\\\sem 7\\\\NNDL\\\\lab\\\\mlp.json'\n",
    "    \n",
    "    X = np.array([[0, 0, 0, 0], [0, 1, 0, 1], [1, 0, 1, 0], [1, 1, 1, 1]])  # Example input\n",
    "    y = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Example output for 2 neurons in output layer\n",
    "\n",
    "    mlp = MLP(config_path)\n",
    "    mlp.train(X, y)\n",
    "    predictions = mlp.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
